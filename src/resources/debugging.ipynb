{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618486e4",
   "metadata": {},
   "source": [
    "## Patient split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7853e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e5e55",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/data/scratch/qc25022/pancreas/tokenised_data_word_level/cprd_upgi\"\n",
    "SPLITS = [\"train\", \"tuning\", \"held_out\"]  # add any extra splits you used\n",
    "\n",
    "def collect_subjects(split):\n",
    "    split_dir = os.path.join(DATA_ROOT, split)\n",
    "    subjects = set()\n",
    "    for fname in os.listdir(split_dir):\n",
    "        if not fname.endswith(\".pkl\"):\n",
    "            continue\n",
    "        with open(os.path.join(split_dir, fname), \"rb\") as f:\n",
    "            for record in pickle.load(f):\n",
    "                subjects.add(record[\"subject_id\"])\n",
    "    return subjects\n",
    "\n",
    "split_subjects = {split: collect_subjects(split) for split in SPLITS}\n",
    "\n",
    "# pairwise intersection report\n",
    "for a in SPLITS:\n",
    "    for b in SPLITS:\n",
    "        if a >= b:\n",
    "            continue\n",
    "        overlap = split_subjects[a] & split_subjects[b]\n",
    "        print(f\"{a} âˆ© {b}: {len(overlap)}\")\n",
    "        if overlap:\n",
    "            print(sorted(list(overlap))[:20], \"...\")  # sample IDs if debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de656e",
   "metadata": {},
   "source": [
    "## Trajectory Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e486faa",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from src.data.unified_dataset import UnifiedEHRDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d6d8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def describe_lengths(split, cutoff):\n",
    "    dataset = UnifiedEHRDataset(\n",
    "        data_dir=DATA_ROOT,\n",
    "        vocab_file=VOCAB,\n",
    "        labels_file=LABELS,\n",
    "        medical_lookup_file=MEDICAL,\n",
    "        lab_lookup_file=LAB,\n",
    "        region_lookup_file=REGION,\n",
    "        time_lookup_file=TIME,\n",
    "        cutoff_months=cutoff,\n",
    "        format=\"text\",\n",
    "        split=split,\n",
    "        max_sequence_length=None,\n",
    "    )\n",
    "    lengths_chars = []\n",
    "    lengths_tokens = []\n",
    "    for item in dataset:\n",
    "        if item is None:\n",
    "            continue\n",
    "        text = item[\"text\"]\n",
    "        lengths_chars.append(len(text))\n",
    "        lengths_tokens.append(len(text.split()))  # crude word count; swap with tokenizer.encode if desired\n",
    "    summary = lambda arr: dict(count=len(arr), mean=np.mean(arr), p95=np.percentile(arr, 95), max=max(arr))\n",
    "    return summary(lengths_chars), summary(lengths_tokens)\n",
    "\n",
    "for split in [\"train\", \"tuning\", \"held_out\"]:\n",
    "    char_stats, token_stats = describe_lengths(split, cutoff=12)\n",
    "    print(f\"{split} char stats: {char_stats}\")\n",
    "    print(f\"{split} token stats: {token_stats}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
